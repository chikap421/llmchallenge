{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX_M57ZPuaJW"
      },
      "source": [
        "## Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w68voy3ZspX3",
        "outputId": "e97fb6bf-42b4-451e-ad16-c97e2668475d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.17.1)\n",
            "Requirement already satisfied: seqeval in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: accelerate in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.27.2)\n",
            "Requirement already satisfied: transformers[torch] in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.38.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: xxhash in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dell g3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] datasets seqeval accelerate -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OKdy-Nz3tcK6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dell g3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from seqeval.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWL0XWFYtmUG",
        "outputId": "55661cf7-799a-40e4-a32b-c75bbc824a0f"
      },
      "outputs": [],
      "source": [
        "# Load a subset of the Universal Dependencies dataset for inspection\n",
        "dataset = load_dataset(\"universal_dependencies\", \"en_gum\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZjbDbjGtcAu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEKi9wRJumKh",
        "outputId": "7e6b3b26-c937-4848-8e13-552923080b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
            "                  idx                                               text  \\\n",
            "0  GUM_academic_art-1            Aesthetic Appreciation and Spanish Art:   \n",
            "1  GUM_academic_art-2                         Insights from Eye-Tracking   \n",
            "2  GUM_academic_art-3  Claire Bailey-Ross claire.bailey-ross@port.ac....   \n",
            "3  GUM_academic_art-4  Andrew Beresford a.m.beresford@durham.ac.uk Du...   \n",
            "4  GUM_academic_art-5  Daniel Smith daniel.smith2@durham.ac.uk Durham...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0    [Aesthetic, Appreciation, and, Spanish, Art, :]   \n",
            "1                     [Insights, from, Eye-Tracking]   \n",
            "2  [Claire, Bailey-Ross, claire.bailey-ross@port....   \n",
            "3  [Andrew, Beresford, a.m.beresford@durham.ac.uk...   \n",
            "4  [Daniel, Smith, daniel.smith2@durham.ac.uk, Du...   \n",
            "\n",
            "                                              lemmas  \\\n",
            "0    [aesthetic, appreciation, and, Spanish, art, :]   \n",
            "1                      [insight, from, eye-tracking]   \n",
            "2  [Claire, Bailey-Ross, claire.bailey-ross@port....   \n",
            "3  [Andrew, Beresford, a.m.beresford@durham.ac.uk...   \n",
            "4  [Daniel, Smith, daniel.smith2@durham.ac.uk, Du...   \n",
            "\n",
            "                                 upos  \\\n",
            "0                  [6, 0, 9, 6, 0, 1]   \n",
            "1                           [0, 2, 0]   \n",
            "2  [10, 10, 10, 10, 2, 10, 1, 10, 10]   \n",
            "3     [10, 10, 10, 10, 10, 1, 10, 10]   \n",
            "4     [10, 10, 10, 10, 10, 1, 10, 10]   \n",
            "\n",
            "                                         xpos  \\\n",
            "0                     [JJ, NN, CC, JJ, NN, :]   \n",
            "1                               [NNS, IN, NN]   \n",
            "2  [NNP, NNP, NNP, NNP, IN, NNP, ,, NNP, NNP]   \n",
            "3      [NNP, NNP, NNP, NNP, NNP, ,, NNP, NNP]   \n",
            "4      [NNP, NNP, NNP, NNP, NNP, ,, NNP, NNP]   \n",
            "\n",
            "                                               feats  \\\n",
            "0  [{'Degree': 'Pos'}, {'Number': 'Sing'}, None, ...   \n",
            "1     [{'Number': 'Plur'}, None, {'Number': 'Sing'}]   \n",
            "2  [{'Number': 'Sing'}, {'Number': 'Sing'}, {'Num...   \n",
            "3  [{'Number': 'Sing'}, {'Number': 'Sing'}, {'Num...   \n",
            "4  [{'Number': 'Sing'}, {'Number': 'Sing'}, {'Num...   \n",
            "\n",
            "                          head  \\\n",
            "0           [2, 0, 5, 5, 2, 2]   \n",
            "1                    [0, 3, 1]   \n",
            "2  [0, 1, 1, 1, 6, 4, 9, 9, 1]   \n",
            "3     [0, 1, 1, 5, 1, 8, 8, 1]   \n",
            "4     [0, 1, 1, 5, 1, 8, 8, 1]   \n",
            "\n",
            "                                              deprel  \\\n",
            "0                [amod, root, cc, amod, conj, punct]   \n",
            "1                                 [root, case, nmod]   \n",
            "2  [root, flat, list, list, case, nmod, punct, am...   \n",
            "3  [root, flat, list, compound, list, punct, amod...   \n",
            "4  [root, flat, list, compound, list, punct, amod...   \n",
            "\n",
            "                                                deps  \\\n",
            "0               [None, None, None, None, None, None]   \n",
            "1                                 [None, None, None]   \n",
            "2  [None, None, None, None, None, None, None, Non...   \n",
            "3   [None, None, None, None, None, None, None, None]   \n",
            "4   [None, None, None, None, None, None, None, None]   \n",
            "\n",
            "                                                misc  \n",
            "0  [{'Discourse': 'preparation:1->57', 'Entity': ...  \n",
            "1  [{'Discourse': 'elaboration:2->1', 'Entity': '...  \n",
            "2  [{'Discourse': 'background:3->57', 'Entity': '...  \n",
            "3  [{'Discourse': 'joint:4->3', 'Entity': '(perso...  \n",
            "4  [{'Discourse': 'joint:5->3', 'Entity': '(perso...  \n",
            "4287\n"
          ]
        }
      ],
      "source": [
        "# Print the column names for the training set\n",
        "print(dataset['train'].column_names)\n",
        "# Convert the 'train' split of your dataset to a Pandas DataFrame\n",
        "train_df = dataset['train'].to_pandas()\n",
        "print(train_df.head())\n",
        "print(len(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rDhDfy27t84l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.0kB/s]\n",
            "c:\\Users\\dell g3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell g3\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "config.json: 100%|██████████| 466/466 [00:00<00:00, 233kB/s]\n",
            "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 7.95MB/s]\n",
            "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 8.28MB/s]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UN1rdA755pY_"
      },
      "outputs": [],
      "source": [
        "unique_pos_tags = set()\n",
        "\n",
        "# Aggregate unique POS tags from all dataset splits\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    for example in dataset[split]:\n",
        "        unique_pos_tags.update(example['upos'])\n",
        "\n",
        "# Now that you have all unique POS tags, create the pos_tag2id and id2pos_tag dictionaries\n",
        "pos_tag2id = {tag: idx for idx, tag in enumerate(unique_pos_tags)}\n",
        "id2pos_tag = {idx: tag for tag, idx in pos_tag2id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NGALWoqXt82P"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", is_split_into_words=True, max_length=128)\n",
        "\n",
        "    labels = [[] for _ in range(len(examples[\"tokens\"]))]  # Initialize labels list for each example\n",
        "    for i, label_sequence in enumerate(examples[\"upos\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Maps tokens to their word\n",
        "        label_ids = [-100] * len(word_ids)  # Initialize all labels to ignore index\n",
        "\n",
        "        previous_word_idx = None\n",
        "        label_index = 0\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is not None and word_idx != previous_word_idx and label_index < len(label_sequence):\n",
        "                label_id = pos_tag2id.get(label_sequence[label_index])  # Use .get() to avoid KeyError\n",
        "                if label_id is not None:  # Check if the label_id exists\n",
        "                    label_ids[word_idx] = label_id\n",
        "                label_index += 1\n",
        "            previous_word_idx = word_idx\n",
        "        labels[i] = label_ids\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657,
          "referenced_widgets": [
            "7f2a384fffe24a13acfed713d8f9223c",
            "7cd17caf9ab443c9890b6595fb563321",
            "94f98261a1854e058ec7a267cd8a9ec9",
            "c9ee5400096942cb9ddf0ec6dfe82328",
            "1341fb902ba944b9a6c85741b6195561",
            "37ebef2815934059852d241daf115b63",
            "d261321bb01946d4acb022b5ef5c3e7a",
            "bc67c9022edb417fafad4361e7fe172c",
            "9030756173574610a95a30a70970e4f1",
            "f2bf93c424d2457481b1de9c0384848a",
            "1174dd3723144528aa4b9e51e79253b1"
          ]
        },
        "id": "NmC2gRs_t8zo",
        "outputId": "2bdeb56d-e998-4528-ee25-2a987b2ee333"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/4287 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 4287/4287 [00:00<00:00, 4834.96 examples/s]\n",
            "Map: 100%|██████████| 784/784 [00:00<00:00, 4904.05 examples/s]\n",
            "Map: 100%|██████████| 890/890 [00:00<00:00, 5254.32 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 4287\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 784\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 890\n",
            "    })\n",
            "})\n",
            "                                           input_ids  \\\n",
            "0  [101, 138, 13051, 13358, 11130, 73784, 74755, ...   \n",
            "1  [101, 61862, 10107, 10188, 28577, 118, 26403, ...   \n",
            "2  [101, 24448, 29761, 118, 16690, 103500, 10112,...   \n",
            "3  [101, 13999, 40575, 62198, 169, 119, 181, 119,...   \n",
            "4  [101, 11792, 11673, 10215, 19428, 119, 39709, ...   \n",
            "\n",
            "                                      attention_mask  \\\n",
            "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
            "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
            "\n",
            "                                              labels  \n",
            "0  [6, 0, 9, 6, 0, 1, -100, -100, -100, -100, -10...  \n",
            "1  [0, 2, 0, -100, -100, -100, -100, -100, -100, ...  \n",
            "2  [10, 10, 10, 10, 2, 10, 1, 10, 10, -100, -100,...  \n",
            "3  [10, 10, 10, 10, 10, 1, 10, 10, -100, -100, -1...  \n",
            "4  [10, 10, 10, 10, 10, 1, 10, 10, -100, -100, -1...  \n",
            "4287\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True, remove_columns=dataset['train'].column_names)\n",
        "print(tokenized_datasets)\n",
        "tokenized_datasets['train']\n",
        "tokenized_datasets_df = tokenized_datasets['train'].to_pandas()\n",
        "print(tokenized_datasets_df.head())\n",
        "print(len(tokenized_datasets_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hp6GrXJ4my9"
      },
      "source": [
        "## Reduce the Dataset Size\n",
        "\n",
        "Given the objective to explore the impact of partial layer freezing on POS tagging performance, starting with a reduced dataset can help quickly gauge the feasibility and potential direction of the research. Once promising configurations are identified,the task can be scaled up to the full dataset to fine-tune and validate the findings.\n",
        "\n",
        "*   **Shuffling**: The datasets are shuffled before reduction to ensure that the reduced dataset is a representative random sample of the original. This is important for maintaining the diversity of examples in smaller datasets.\n",
        "    \n",
        "*   **Seed for Reproducibility**: The `seed=42` parameter in the `.shuffle()` method ensures that the shuffling process is reproducible, meaning you'll get the same reduced dataset if you run the code multiple times.\n",
        "    \n",
        "*   **Adjusting Reduction Percentage**: You can modify `reduced_percentage` to any value between 0 and 1 (exclusive) depending on how much you want to reduce the dataset by. A value of `0.5` reduces the dataset to half its original size, while `0.1` would reduce it to 10%, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqeVHmFp4q3N",
        "outputId": "a5f26cff-cba0-484f-c0dd-781eba4d47a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 428\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 78\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 89\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "reduced_percentage = 0.1  # Target percentage to reduce each dataset to\n",
        "\n",
        "# Function to reduce dataset size by a specified percentage\n",
        "def reduce_dataset_size(dataset, percentage):\n",
        "    # Calculate the number of samples to select based on the reduction percentage\n",
        "    num_samples = int(len(dataset) * percentage)\n",
        "    # Shuffle the dataset (with a fixed seed for reproducibility) and select the first num_samples\n",
        "    reduced_dataset = dataset.shuffle(seed=42).select(range(num_samples))\n",
        "    return reduced_dataset\n",
        "\n",
        "# Reduce the size of the training, validation, and test datasets\n",
        "reduced_train_dataset = reduce_dataset_size(tokenized_datasets['train'], reduced_percentage)\n",
        "reduced_validation_dataset = reduce_dataset_size(tokenized_datasets['validation'], reduced_percentage)\n",
        "reduced_test_dataset = reduce_dataset_size(tokenized_datasets['test'], reduced_percentage)\n",
        "\n",
        "# Create a new DatasetDict with the reduced datasets\n",
        "reduced_tokenized_datasets = DatasetDict({\n",
        "    'train': reduced_train_dataset,\n",
        "    'validation': reduced_validation_dataset,\n",
        "    'test': reduced_test_dataset\n",
        "})\n",
        "\n",
        "# Now, you can proceed with training and evaluation using `reduced_tokenized_datasets`\n",
        "reduced_tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqtvuYbi4rOj"
      },
      "source": [
        "## Define the Compute Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4PRjxurYjr3v"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    # Flatten both the predictions and labels for evaluation\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2pos_tag[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "        [id2pos_tag[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    # Flatten the lists\n",
        "    true_predictions = [p for sublist in true_predictions for p in sublist]\n",
        "    true_labels = [l for sublist in true_labels for l in sublist]\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='macro', zero_division=0)\n",
        "    accuracy = accuracy_score(true_labels, true_predictions)\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'accuracy': accuracy,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X6ZdyoEtiAm"
      },
      "source": [
        "## Dataset Preparation and Baseline Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "iOXPugr5tDci",
        "outputId": "234fd476-f1e1-40aa-bf90-7f259c6061bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "model.safetensors: 100%|██████████| 542M/542M [01:04<00:00, 8.35MB/s] \n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " 33%|███▎      | 4/12 [01:45<03:08, 23.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.7127, 'grad_norm': 1.5092084407806396, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                              \n",
            " 33%|███▎      | 4/12 [01:53<03:08, 23.56s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.534414291381836, 'eval_precision': 0.040135669869983036, 'eval_recall': 0.06133902329359544, 'eval_f1': 0.023910133503189653, 'eval_accuracy': 0.19053051911009697, 'eval_runtime': 7.6576, 'eval_samples_per_second': 10.186, 'eval_steps_per_second': 1.306, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 8/12 [03:48<01:42, 25.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4788, 'grad_norm': 1.0170406103134155, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                              \n",
            " 67%|██████▋   | 8/12 [03:55<01:42, 25.67s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.458814859390259, 'eval_precision': 0.03611142973498478, 'eval_recall': 0.06880594193542265, 'eval_f1': 0.03770750561567728, 'eval_accuracy': 0.2002281802624073, 'eval_runtime': 7.7464, 'eval_samples_per_second': 10.069, 'eval_steps_per_second': 1.291, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [05:40<00:00, 24.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4034, 'grad_norm': 1.0129964351654053, 'learning_rate': 0.0, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                               \n",
            "100%|██████████| 12/12 [05:46<00:00, 24.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.438114643096924, 'eval_precision': 0.0748533313937097, 'eval_recall': 0.073749851011053, 'eval_f1': 0.044792428413757863, 'eval_accuracy': 0.20365088419851682, 'eval_runtime': 6.6827, 'eval_samples_per_second': 11.672, 'eval_steps_per_second': 1.496, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [05:56<00:00, 29.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 356.5815, 'train_samples_per_second': 3.601, 'train_steps_per_second': 0.034, 'train_loss': 2.531620184580485, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Assuming pos_tag2id has been defined based on your dataset's unique POS tags\n",
        "num_labels = len(pos_tag2id)\n",
        "\n",
        "model_baseline = DistilBertForTokenClassification.from_pretrained(\n",
        "    'distilbert-base-multilingual-cased',\n",
        "    num_labels=num_labels\n",
        ")\n",
        "\n",
        "training_args_baseline = TrainingArguments(\n",
        "    output_dir='./baseline_results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=128,  # Updated batch size\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer_baseline = Trainer(\n",
        "    model=model_baseline,\n",
        "    args=training_args_baseline,\n",
        "    train_dataset=reduced_tokenized_datasets['train'],\n",
        "    eval_dataset=reduced_tokenized_datasets['validation'],\n",
        "    compute_metrics=compute_metrics  # Ensure this function computes accuracy, precision, recall, and F1\n",
        ")\n",
        "\n",
        "# Train the baseline model\n",
        "trainer_baseline.train()\n",
        "\n",
        "# Save the baseline model\n",
        "trainer_baseline.save_model(\"./baseline_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StafW9ZDtXvF"
      },
      "source": [
        "## Model Adjustment and Partial Freezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6YJCE4NqM03n"
      },
      "outputs": [],
      "source": [
        "def train_with_different_freezing_configs(freeze_configs, training_args, compute_metrics, train_dataset, eval_dataset):\n",
        "    results = {}\n",
        "    \n",
        "    for config in freeze_configs:\n",
        "        model_adjusted = initialize_and_freeze_model(layers_to_freeze=config)\n",
        "        \n",
        "        trainer = Trainer(\n",
        "            model=model_adjusted,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            compute_metrics=compute_metrics\n",
        "        )\n",
        "\n",
        "        config_name = \"_\".join(str(x) for x in config)  # Convert layer indices to a string\n",
        "        print(f\"Training with layers {config} frozen...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        eval_results = trainer.evaluate()\n",
        "        results[f\"Layers frozen: {config}\"] = eval_results\n",
        "\n",
        "        # Save the model for each configuration\n",
        "        model_save_path = f\"./models/model_layers_{config_name}_frozen\"\n",
        "        model_adjusted.save_pretrained(model_save_path)\n",
        "        \n",
        "        # Optionally, save the Trainer state as well\n",
        "        trainer.save_state()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GAayByDNLaue"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with layers [0] frozen...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [03:51<04:48, 28.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.7242, 'grad_norm': 1.6362910270690918, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A                                         \n",
            "                                              \n",
            " 17%|█▋        | 2/12 [03:59<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-4 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.5439460277557373, 'eval_precision': 0.07232232514092295, 'eval_recall': 0.06425808009638541, 'eval_f1': 0.035498534805695266, 'eval_accuracy': 0.18824871648602395, 'eval_runtime': 7.8122, 'eval_samples_per_second': 9.984, 'eval_steps_per_second': 1.28, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [05:54<04:48, 28.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4932, 'grad_norm': 1.1228306293487549, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                           \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [06:02<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-8 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.459272623062134, 'eval_precision': 0.09910206939793172, 'eval_recall': 0.07040431012361928, 'eval_f1': 0.04190813099942325, 'eval_accuracy': 0.20193953223046207, 'eval_runtime': 7.876, 'eval_samples_per_second': 9.903, 'eval_steps_per_second': 1.27, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [07:51<04:48, 28.82s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4166, 'grad_norm': 1.1574232578277588, 'learning_rate': 0.0, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [08:00<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-12 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.4365015029907227, 'eval_precision': 0.1277376218552689, 'eval_recall': 0.07589225702768687, 'eval_f1': 0.049141724447015776, 'eval_accuracy': 0.20821448944666288, 'eval_runtime': 8.2136, 'eval_samples_per_second': 9.496, 'eval_steps_per_second': 1.217, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 12/12 [05:55<00:00, 29.62s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 355.4272, 'train_samples_per_second': 3.613, 'train_steps_per_second': 0.034, 'train_loss': 2.5446723302205405, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.43it/s]\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with layers [0, 1] frozen...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [09:50<04:48, 28.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.6893, 'grad_norm': 1.4679749011993408, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                           \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [09:57<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-4 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.54581356048584, 'eval_precision': 0.11086207639849797, 'eval_recall': 0.0651563655168822, 'eval_f1': 0.03586913712262081, 'eval_accuracy': 0.1876782658300057, 'eval_runtime': 7.249, 'eval_samples_per_second': 10.76, 'eval_steps_per_second': 1.379, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [11:36<04:48, 28.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4983, 'grad_norm': 1.134790062904358, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                           \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [11:44<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-8 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.467076063156128, 'eval_precision': 0.07514311406303598, 'eval_recall': 0.06643429871721543, 'eval_f1': 0.035471896789406925, 'eval_accuracy': 0.195094124358243, 'eval_runtime': 7.5971, 'eval_samples_per_second': 10.267, 'eval_steps_per_second': 1.316, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [13:21<04:48, 28.82s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.4244, 'grad_norm': 1.085565447807312, 'learning_rate': 0.0, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                            \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [13:28<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-12 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.4438469409942627, 'eval_precision': 0.08206694541753544, 'eval_recall': 0.07173085503249432, 'eval_f1': 0.04283320928335669, 'eval_accuracy': 0.20422133485453509, 'eval_runtime': 7.099, 'eval_samples_per_second': 10.987, 'eval_steps_per_second': 1.409, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 12/12 [05:19<00:00, 26.63s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 319.5556, 'train_samples_per_second': 4.018, 'train_steps_per_second': 0.038, 'train_loss': 2.5373409589131675, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with layers [0, 1, 2] frozen...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            " 17%|█▋        | 2/12 [15:16<04:48, 28.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.703, 'grad_norm': 1.5147755146026611, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A\n",
            "\u001b[A                                           \n",
            "\n",
            "                                               \n",
            " 17%|█▋        | 2/12 [15:24<04:48, 28.82s/it]\n",
            "\u001b[ACheckpoint destination directory ./baseline_results\\checkpoint-4 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.5697734355926514, 'eval_precision': 0.07895034661380605, 'eval_recall': 0.06430688495333214, 'eval_f1': 0.03632615090725487, 'eval_accuracy': 0.18254420992584142, 'eval_runtime': 7.7185, 'eval_samples_per_second': 10.106, 'eval_steps_per_second': 1.296, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# Corrected function to initialize and freeze specified layers of the model\n",
        "def initialize_and_freeze_model(layers_to_freeze):\n",
        "    # Initialize the model\n",
        "    model = DistilBertForTokenClassification.from_pretrained(\n",
        "        'distilbert-base-multilingual-cased',\n",
        "        num_labels=num_labels  # Ensure num_labels is defined correctly\n",
        "    )\n",
        "\n",
        "    # Freeze specified layers\n",
        "    for name, param in model.named_parameters():\n",
        "        # Freeze layers based on the provided list\n",
        "        if any(f\"transformer.layer.{i}.\" in name for i in layers_to_freeze):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define different freezing configurations considering DistilBERT has 6 layers\n",
        "freeze_configs = [\n",
        "    [0],                # Freeze only the first layer\n",
        "    [0, 1],             # Freeze the first two layers\n",
        "    [0, 1, 2],          # Freeze the first three layers\n",
        "    [0, 1, 2, 3],       # Freeze the first four layers\n",
        "    [0, 1, 2, 3, 4],    # Freeze the first five layers\n",
        "    [0, 1, 2, 3, 4, 5]  # Freeze all layers (for completeness of experiment)\n",
        "]\n",
        "\n",
        "\n",
        "# Proceed with the experiments\n",
        "experiment_results = train_with_different_freezing_configs(\n",
        "    freeze_configs=freeze_configs,\n",
        "    training_args=training_args_baseline,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=reduced_tokenized_datasets['train'],\n",
        "    eval_dataset=reduced_tokenized_datasets['validation']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS6_xYrStyWn"
      },
      "source": [
        "## Analysis and Comaprison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J_G5XkPtmTm"
      },
      "outputs": [],
      "source": [
        "# # Evaluate both models on the test set to compare performance\n",
        "# baseline_results = trainer_baseline.evaluate(tokenized_datasets['test'])\n",
        "# experiment_results = trainer_adjusted.evaluate(tokenized_datasets['test'])\n",
        "\n",
        "# print(\"Baseline Model Performance:\", baseline_results)\n",
        "# print(\"Adjusted Model Performance:\", experiment_results)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1174dd3723144528aa4b9e51e79253b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1341fb902ba944b9a6c85741b6195561": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ebef2815934059852d241daf115b63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd17caf9ab443c9890b6595fb563321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ebef2815934059852d241daf115b63",
            "placeholder": "​",
            "style": "IPY_MODEL_d261321bb01946d4acb022b5ef5c3e7a",
            "value": "Map: 100%"
          }
        },
        "7f2a384fffe24a13acfed713d8f9223c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7cd17caf9ab443c9890b6595fb563321",
              "IPY_MODEL_94f98261a1854e058ec7a267cd8a9ec9",
              "IPY_MODEL_c9ee5400096942cb9ddf0ec6dfe82328"
            ],
            "layout": "IPY_MODEL_1341fb902ba944b9a6c85741b6195561"
          }
        },
        "9030756173574610a95a30a70970e4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94f98261a1854e058ec7a267cd8a9ec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc67c9022edb417fafad4361e7fe172c",
            "max": 784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9030756173574610a95a30a70970e4f1",
            "value": 784
          }
        },
        "bc67c9022edb417fafad4361e7fe172c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ee5400096942cb9ddf0ec6dfe82328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bf93c424d2457481b1de9c0384848a",
            "placeholder": "​",
            "style": "IPY_MODEL_1174dd3723144528aa4b9e51e79253b1",
            "value": " 784/784 [00:00&lt;00:00, 1390.68 examples/s]"
          }
        },
        "d261321bb01946d4acb022b5ef5c3e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2bf93c424d2457481b1de9c0384848a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
